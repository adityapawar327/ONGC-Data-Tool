# ğŸš€ ONGC Data Toolkit

<div align="center">
  <img src="public/ongc (1).jpg" alt="ONGC Logo" width="200"/>
  
  <h3>Enterprise Data Management & Analysis Platform</h3>
  
  <p>
    <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python 3.8+"/>
    <img src="https://img.shields.io/badge/Streamlit-1.24+-FF4B4B.svg" alt="Streamlit"/>
    <img src="https://img.shields.io/badge/PostgreSQL-13+-336791.svg" alt="PostgreSQL"/>
    <img src="https://img.shields.io/badge/DeepSeek-R1-8B-00A67E.svg" alt="DeepSeek-R1"/>
  </p>
</div>

## ğŸ“‹ Table of Contents
- [Overview](#-overview)
- [Core Technologies](#-core-technologies)
- [Key Features](#-key-features)
- [Installation Guide](#-installation-guide)
- [Project Structure](#-project-structure)
- [Security & Performance](#-security--performance)
- [Contributing](#-contributing)

## ğŸŒŸ Overview

An enterprise-grade data management and analysis platform built with Python and Streamlit, specifically designed for processing geoscientific and survey datasets. This toolkit leverages advanced AI/ML capabilities, robust data processing pipelines, and modern software architecture patterns to deliver a comprehensive solution for data management challenges.

## ğŸ› ï¸ Core Technologies

<div align="center">
  <table>
    <tr>
      <td align="center"><b>Frontend</b></td>
      <td align="center"><b>Backend</b></td>
      <td align="center"><b>Database</b></td>
      <td align="center"><b>AI/ML</b></td>
    </tr>
    <tr>
      <td>Streamlit, Custom CSS</td>
      <td>Python 3.8+, SQLAlchemy</td>
      <td>PostgreSQL</td>
      <td>DeepSeek-R1, LangChain</td>
    </tr>
  </table>
</div>

### ğŸ§  AI/ML Stack
- **ğŸ¤– RAG Architecture** - Retrieval Augmented Generation
- **ğŸ”— LangChain** - LLM Orchestration
- **ğŸ§® DeepSeek-R1** - 8B Parameter Model
- **ğŸ” FAISS** - Vector Store for Semantic Search
- **ğŸš€ Ollama** - Local LLM Deployment

### ğŸ“Š Data Processing
- **ğŸ¼ Pandas** - Data Manipulation
- **ğŸ”¢ NumPy** - Numerical Computing
- **ğŸ“ Python-DOCX** - Document Processing
- **ğŸ” Fuzzy Search** - Custom Implementation

## ğŸ¯ Key Features

### ğŸ“Š Enterprise Data Management

* **Intelligent Data Import**
  * PostgreSQL Integration with Transaction Support
  * Smart Schema Detection and Mapping
  * Automated Data Type Inference
  * Duplicate Detection and Resolution

* **Advanced Data Cleaning Pipeline**
  * Automated Data Quality Assessment
  * Multi-stage Cleaning Operations
  * Custom Validation Rules Engine
  * Batch Processing Support

* **Format Standardization**
  * Cross-format Schema Matching
  * Intelligent Data Normalization
  * Custom Template Support
  * Automated Data Transformation

### ğŸ” Analysis & Intelligence

* **Enterprise Search**
  * Vector-based Semantic Search
  * Fuzzy Matching Algorithm
  * Multi-field Search Support
  * Configurable Relevance Scoring

* **Data Analysis Tools**
  * Real-time Data Visualization
  * Pattern Detection Engine
  * Statistical Analysis
  * Anomaly Detection

* **AI-Powered Features**
  * Context-Aware RAG System
  * Natural Language Query Processing
  * Automated Data Quality Reports
  * Intelligent Data Classification

### ğŸ› ï¸ Technical Architecture

* **Modular Design**
  * Component-based Architecture
  * Clean Code Principles
  * Extensive Error Handling
  * Comprehensive Logging

* **Security**
  * Local LLM Deployment
  * Zero Data Leakage
  * Secure Database Operations
  * Input Validation

* **Performance**
  * Optimized Data Processing
  * Efficient Memory Management
  * Batch Operation Support
  * Caching Mechanisms

## ğŸ”§ Installation Guide

<div align="center">
  <img src="https://img.shields.io/badge/Setup-Guide-00A67E.svg" alt="Setup Guide"/>
</div>

### ğŸ“‹ Prerequisites

1. **ğŸ Python Setup**
   ```bash
   # Install Python 3.8 or later from https://www.python.org/downloads/
   # Verify installation
   python --version
   ```

2. **ğŸ˜ PostgreSQL Installation**
   - Download and install PostgreSQL 13+ from https://www.postgresql.org/download/
   - During installation, note down your password
   - Create a new database:
     ```sql
     CREATE DATABASE ongcdata;
     ```

3. **ğŸ¤– Ollama Setup for DeepSeek**
   ```bash
   # Install Ollama from https://ollama.ai/download
   
   # Pull the latest DeepSeek-R1 model (8B parameters)
   ollama pull deepseek-r1

   # Verify installation
   ollama list
   ```

### Project Setup

1. **Clone & Navigate**
   ```bash
   git clone https://github.com/adityapawar327/ONGC-Data-Tool.git
   cd ongc_app
   ```

2. **Virtual Environment**
   ```bash
   # Create virtual environment
   python -m venv venv

   # Activate (Windows)
   .\venv\Scripts\activate

   # Activate (Linux/Mac)
   source venv/bin/activate
   ```

3. **Install Dependencies**
   ```bash
   # Update pip
   python -m pip install --upgrade pip

   # Install required packages
   pip install -r requirements.txt
   ```

4. **Configure Database**
   - Open `app.py`
   - Update the DATABASE_URL:
     ```python
     DATABASE_URL = "postgresql://username:password@localhost:5432/ongcdata"
     ```
   - Replace username, password with your PostgreSQL credentials

5. **Environment Setup**
   ```bash
   # Create .env file
   echo DATABASE_URL=postgresql://username:password@localhost:5432/ongcdata > .env
   echo OLLAMA_BASE_URL=http://localhost:11434 >> .env
   ```

6. **Run Application**
   ```bash
   # Start Ollama in background
   ollama serve

   # In a new terminal, run the application
   streamlit run app.py
   ```

### Verification Steps

1. **Check Database Connection**
   - Application will show successful database connection on startup
   - Test data upload functionality

2. **Verify AI Features**
   - Go to "AI Assistant" section
   - Type a test query
   - System should respond using local DeepSeek model

3. **Test Data Processing**
   - Upload a sample Excel/CSV file
   - Verify data cleaning features
   - Test search functionality

### Troubleshooting

1. **Database Issues**
   - Verify PostgreSQL service is running
   - Check database credentials
   - Ensure database exists

2. **AI Model Issues**
   - Verify Ollama is running: `curl http://localhost:11434/api/tags`
   - Check DeepSeek model: `ollama list`
   - Try restarting Ollama service

3. **Application Errors**
   - Check Python version compatibility
   - Verify all dependencies are installed
   - Check console for error messages

## ğŸ“¦ Project Structure

<div align="center">
  <img src="https://img.shields.io/badge/Project-Structure-00A67E.svg" alt="Project Structure"/>
</div>

```
ongc_app/
â”œâ”€â”€ ğŸ“ app.py              # Main application entry point
â”œâ”€â”€ ğŸ¤– ai.py              # RAG system and LLM integration
â”œâ”€â”€ ğŸ“Š analyze.py         # Data analysis engine
â”œâ”€â”€ ğŸ§¹ cleaning.py        # Data cleaning pipeline
â”œâ”€â”€ ğŸ”„ compare.py         # File comparison logic
â”œâ”€â”€ ğŸ“ convert.py         # Format conversion utilities
â”œâ”€â”€ ğŸ“ˆ data_type_wise.py  # Data classification system
â”œâ”€â”€ ğŸ·ï¸ label.py           # Document generation
â”œâ”€â”€ ğŸ”— link.py           # Data linking engine
â”œâ”€â”€ ğŸ” searching.py      # Search implementation
â”œâ”€â”€ ğŸ“ public/           # Static assets directory
â”œâ”€â”€ ğŸ³ .devcontainer/    # Development container configuration
â”œâ”€â”€ ğŸ“ temp/            # Temporary files directory
â””â”€â”€ ğŸ“‹ requirements.txt  # Project dependencies
```

## ğŸ“š Module Descriptions

### ğŸ¤– AI Module (`ai.py`)
The AI engine implements a sophisticated RAG (Retrieval Augmented Generation) system that powers the intelligent features of the toolkit:
- Context-aware natural language processing
- Document chunking and vectorization
- Semantic search capabilities
- Custom prompt engineering for domain-specific tasks
- Integration with DeepSeek-R1 model for advanced reasoning

### ğŸ“Š Analysis Module (`analyze.py`)
Comprehensive data analysis engine that provides:
- Statistical analysis of datasets
- Pattern detection and trend analysis
- Data quality assessment
- Automated report generation
- Visualization capabilities for insights

### ğŸ§¹ Cleaning Module (`cleaning.py`)
Advanced data cleaning pipeline that handles:
- Automated data quality checks
- Missing value treatment
- Data type standardization
- Text normalization
- Duplicate detection and removal
- Custom validation rules

### ğŸ”„ Compare Module (`compare.py`)
File comparison utility that enables:
- Cell-by-cell comparison of datasets
- Visual highlighting of differences
- Structure comparison
- Version tracking
- Change detection

### ğŸ“ Convert Module (`convert.py`)
File format conversion utility that supports:
- CSV to Excel conversion
- Excel to CSV conversion
- DOCX to PDF conversion
- Format preservation
- Batch processing capabilities

### ğŸ“ˆ Data Type Module (`data_type_wise.py`)
Specialized data classification system that provides:
- Automatic data type detection
- Schema validation
- Data categorization
- Format standardization
- Metadata extraction

### ğŸ·ï¸ Label Module (`label.py`)
Document generation system for creating:
- Physical media labels
- Standardized documentation
- Custom templates
- Batch label generation
- Format-specific outputs

### ğŸ”— Link Module (`link.py`)
Data linking engine that enables:
- Cross-dataset relationships
- Schema matching
- Data normalization
- Relationship mapping
- Link validation

### ğŸ” Search Module (`searching.py`)
Advanced search implementation featuring:
- Fuzzy matching algorithms
- Multi-field search
- Relevance scoring
- Pattern matching
- Search result highlighting

### ğŸ“ Public Directory
Contains static assets:
- Application logos
- UI elements
- Documentation resources
- Template files

### ğŸ³ DevContainer Directory
Development environment configuration:
- Container setup
- Development tools
- Environment variables
- Build configurations

### ğŸ“ Temp Directory
Temporary file storage:
- Processing cache
- Temporary outputs
- Session data
- Working files

## ğŸ”’ Security & Performance

<div align="center">
  <img src="https://img.shields.io/badge/Security-Performance-00A67E.svg" alt="Security & Performance"/>
</div>

- **ğŸ›¡ï¸ Zero-Trust Architecture**
  * All AI operations run locally
  * No data leaves your system
  * Secure database operations

- **âš¡ Performance Optimizations**
  * Efficient memory management
  * Batch processing capabilities
  * Caching mechanisms
  * Parallel processing support

## ğŸ¤ Contributing

<div align="center">
  <img src="https://img.shields.io/badge/Contributing-Guide-00A67E.svg" alt="Contributing Guide"/>
</div>

We follow a standardized commit convention. See [COMMIT_CONVENTION.md](COMMIT_CONVENTION.md) for guidelines.

## ğŸ“« Contact

For questions or feedback, please contact the project maintainer.

## ğŸ“„ License

This project is intended for internal ONGC use. For licensing or external use, please contact the project maintainer.

---
**Keywords**: Python, Data Engineering, Machine Learning, RAG, LangChain, FAISS, Vector Search, ETL, Data Processing, Enterprise Software, Data Analysis, Natural Language Processing, PostgreSQL, Streamlit, Software Architecture, Data Pipeline, Data Quality, Business Intelligence

<div align="center">
  <p>Built with â¤ï¸ for ONGC</p>
  <p>Â© 2024 ONGC Data Toolkit</p>
</div>
