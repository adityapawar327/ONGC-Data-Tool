# ğŸš€ ONGC Data Toolkit

An enterprise-grade data management and analysis platform built with Python and Streamlit, specifically designed for processing geoscientific and survey datasets. This toolkit leverages advanced AI/ML capabilities, robust data processing pipelines, and modern software architecture patterns to deliver a comprehensive solution for data management challenges.

## ğŸŒŸ Core Technologies

- **Frontend**: Streamlit, Custom CSS
- **Backend**: Python 3.8+, SQLAlchemy
- **Database**: PostgreSQL
- **AI/ML Stack**: 
  - RAG (Retrieval Augmented Generation) Architecture
  - LangChain for LLM Orchestration
  - DeepSeek-Coder-1.3B (8B parameters) for Intelligence
  - FAISS Vector Store for Semantic Search
  - Ollama for Local LLM Deployment
- **Data Processing**: Pandas, NumPy
- **Search Engine**: Custom Fuzzy Search with Levenshtein Distance
- **Document Processing**: Python-DOCX, PDF Generation

## ğŸ¯ Key Features

### ğŸ“Š Enterprise Data Management

* **Intelligent Data Import**
  * PostgreSQL Integration with Transaction Support
  * Smart Schema Detection and Mapping
  * Automated Data Type Inference
  * Duplicate Detection and Resolution

* **Advanced Data Cleaning Pipeline**
  * Automated Data Quality Assessment
  * Multi-stage Cleaning Operations
  * Custom Validation Rules Engine
  * Batch Processing Support

* **Format Standardization**
  * Cross-format Schema Matching
  * Intelligent Data Normalization
  * Custom Template Support
  * Automated Data Transformation

### ğŸ” Analysis & Intelligence

* **Enterprise Search**
  * Vector-based Semantic Search
  * Fuzzy Matching Algorithm
  * Multi-field Search Support
  * Configurable Relevance Scoring

* **Data Analysis Tools**
  * Real-time Data Visualization
  * Pattern Detection Engine
  * Statistical Analysis
  * Anomaly Detection

* **AI-Powered Features**
  * Context-Aware RAG System
  * Natural Language Query Processing
  * Automated Data Quality Reports
  * Intelligent Data Classification

### ğŸ› ï¸ Technical Architecture

* **Modular Design**
  * Component-based Architecture
  * Clean Code Principles
  * Extensive Error Handling
  * Comprehensive Logging

* **Security**
  * Local LLM Deployment
  * Zero Data Leakage
  * Secure Database Operations
  * Input Validation

* **Performance**
  * Optimized Data Processing
  * Efficient Memory Management
  * Batch Operation Support
  * Caching Mechanisms

## ğŸ”§ Installation

```bash
# Clone the repository
git clone https://github.com/adityapawar327/ONGC-Data-Tool.git
cd ongc_app

# Create and activate virtual environment (Windows)
python -m venv venv
.\\venv\\Scripts\\activate

# Install dependencies
pip install -r requirements.txt

# Configure PostgreSQL
# Update DATABASE_URL in app.py with your credentials

# Run the application
streamlit run app.py
```

## ğŸ“¦ Project Structure

```
ongc_app/
â”œâ”€â”€ app.py              # Main application entry point
â”œâ”€â”€ ai.py              # RAG system and LLM integration
â”œâ”€â”€ analyze.py         # Data analysis engine
â”œâ”€â”€ cleaning.py        # Data cleaning pipeline
â”œâ”€â”€ compare.py         # File comparison logic
â”œâ”€â”€ convert.py         # Format conversion utilities
â”œâ”€â”€ data_type_wise.py  # Data classification system
â”œâ”€â”€ label.py           # Document generation
â”œâ”€â”€ link.py           # Data linking engine
â”œâ”€â”€ searching.py      # Search implementation
â””â”€â”€ requirements.txt  # Project dependencies
```

## ğŸ’¡ Key Components

### AI Engine (`ai.py`)
- Implements enterprise-grade RAG architecture
- Integrates DeepSeek-Coder-1.3B LLM
- Custom prompt engineering for domain-specific tasks
- FAISS vector store for efficient similarity search

### Data Processing (`cleaning.py`, `analyze.py`)
- Advanced data cleaning pipelines
- Statistical analysis tools
- Pattern detection algorithms
- Data quality assessment

### Search Engine (`searching.py`)
- Custom fuzzy search implementation
- Levenshtein distance calculations
- Multi-field search capability
- Relevance scoring system

## ğŸ”’ Security & Performance

- **Zero-Trust Architecture**
  * All AI operations run locally
  * No data leaves your system
  * Secure database operations

- **Performance Optimizations**
  * Efficient memory management
  * Batch processing capabilities
  * Caching mechanisms
  * Parallel processing support

## ğŸ¤ Contributing

We follow a standardized commit convention. See [COMMIT_CONVENTION.md](COMMIT_CONVENTION.md) for guidelines.

## ğŸ“« Contact

For questions or feedback, please contact the project maintainer.

## ğŸ“„ License

This project is intended for internal ONGC use. For licensing or external use, please contact the project maintainer.

---
**Keywords**: Python, Data Engineering, Machine Learning, RAG, LangChain, FAISS, Vector Search, ETL, Data Processing, Enterprise Software, Data Analysis, Natural Language Processing, PostgreSQL, Streamlit, Software Architecture, Data Pipeline, Data Quality, Business Intelligence
